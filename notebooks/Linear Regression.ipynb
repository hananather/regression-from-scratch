{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8d23478b-2bea-46ae-9570-702d12f9c711",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06645b33-00f2-4f7b-a3d3-636391680495",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressionScratch:\n",
    "    \"\"\"\n",
    "    The linear regression model implemented from scratch\n",
    "    \"\"\"\n",
    "    def __init__(self, num_inputs, lr, sigma = 0.01):\n",
    "        self.w = torch.normal(0, sigma, (num_inputs,1), requires_grad = True)\n",
    "        self.b = torch.zeros(1, requires_grad = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d061d3-bbf8-4bb5-aa3b-05048d037629",
   "metadata": {},
   "source": [
    "We must define our model, relating its inputs and parameters to the output. For our linear model we simply take the maxtrix-vector product of the input features $X$ and the model weights $w$, and add the offset $b$ to each example. \n",
    "\n",
    "The product $Xw$ is a vector and $b$ is a scalar, because of the broadcasting mechanisn, when we add a vector  and a scalar, the scalar is added to each component of the vector\n",
    "\n",
    "Updating our model requires taking the gradient of the loss function. We use the squared loss function. We will return the averaged loss value among all examples in the **minibatch**\n",
    "\n",
    "The `configure_optimizers` method will return an instance of the SGD class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b5e9784d-16a4-40ba-8f7c-55948ce91bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressionScratch:\n",
    "    \"\"\"\n",
    "    The linear regression model implemented from scratch\n",
    "    \"\"\"\n",
    "    def __init__(self, num_inputs, lr, sigma = 0.01):\n",
    "        self.w = torch.normal(0, sigma, (num_inputs,1), requires_grad = True)\n",
    "        self.b = torch.zeros(1, requires_grad = True)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return torch.matmul(X, self.w) + self.b\n",
    "\n",
    "    def loss(self, y_hat, y):\n",
    "        l = (y_hat - y) ** 2 / 2\n",
    "        return l.mean()\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return SGD([self.w, self.b], self.lr)\n",
    "                  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be8b2de-6c74-46b0-89c8-7ae22b1a2dd6",
   "metadata": {},
   "source": [
    "###  Optimization Algo\n",
    "\n",
    "Linear regression has a closed-form solution, however, our goal here is to illustrate how to train a more general neural networks, which requires minibatch SGD.\n",
    "\n",
    "- at each step..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "28113691-d474-44cb-a1d8-199b1446f09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    \"\"\"\n",
    "    Minibatch stochastic gradient descent\n",
    "    \"\"\"\n",
    "    def __init__(self, params, lr):\n",
    "        self.params = params\n",
    "        self.lr = lr\n",
    "        \n",
    "    def step(self):\n",
    "        for param in self.params:\n",
    "            param -= self.lr * param.grad\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        for param in self.params:\n",
    "            if param.grad is not None:\n",
    "                param.grad.zero_()\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6410baa7-ba35-46b9-b825-54f3a4977539",
   "metadata": {},
   "source": [
    "### Training Loop\n",
    "\n",
    "- in each epoch, we iterate through the entire training dataset, passing once through every example (assuming the number of examples is divisible by batch size)\n",
    "- in each **iteration** we grab a minibatch of training examples and compute the loss through the `training_step` method. We compute the gradients with with respect to each parameter; call the optimization algorithm to update the model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "a09fbd96-e1d6-4ec2-b572-1458190ed30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SyntheticRegressionData:\n",
    "    \"\"\"\n",
    "    Synthetic data for linear regression\n",
    "    \"\"\"\n",
    "    def __init__(self, w, b, noise= 0.01, num_train = 1000, num_val = 1000, \n",
    "                 batch_size = 32):\n",
    "        self.num_train = num_train\n",
    "        self.num_val = num_val\n",
    "        self.batch_size = batch_size\n",
    "        n = num_train + num_val\n",
    "        self.X  = torch.randn(n, len(w))\n",
    "        noise = torch.randn(n, 1) * noise\n",
    "        self.y = torch.matmul(self.X, w.reshape((-1,1))) + b + noise\n",
    "\n",
    "    def get_dataloader(self, train):\n",
    "        if train:\n",
    "            indices = list(range(0, self.num_train))\n",
    "            random.shuffle(indices)\n",
    "        else: \n",
    "            indices = list(range(self.num_train, self.num_train + self.num_val))\n",
    "        for i in range(0, len(indices), self.batch_size):\n",
    "            batch_indices = torch.tensor(indices[i: i+ self.batch_size])\n",
    "            yield self.X[batch_indices], self.y[batch_indices]\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "cf969add-972b-4bc3-8e97-75d9da5db7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = SyntheticRegressionData(w= torch.tensor([2, -3.4]), b = 4.2 )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4ccdf0-44bb-4718-a574-b07979d0128f",
   "metadata": {},
   "source": [
    "Training machine learning models often requires multiple passes over the data, grabbing one minibatch of the examples at a time. This data is used to update the model. \n",
    "\n",
    "`get_dataloader` method takes a batch size, a matrix of features, and a vector of labels, and generates minibatche of size `batch_size`.\n",
    "\n",
    "As such each minibatch consists of a tuple of features and labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "dc2b3c7d-5767-41ca-9f98-b82734bcf802",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: torch.Size([32, 2]) \n",
      "y shape: torch.Size([32, 1])\n"
     ]
    }
   ],
   "source": [
    "X , y = next(iter(data.get_dataloader(True)))\n",
    "print('X shape:', X.shape, '\\ny shape:', y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "4007eafc-ced4-416f-8fac-42ce23c60c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, max_epochs, num_gpus, gradient_clip_val=0):\n",
    "        assert num_gpus == 0, 'no GPU support yet'\n",
    "\n",
    "    def prepare_data(self, data):\n",
    "        self.train_dataloader = data.train_dataloader()\n",
    "        self.val_dataloader = data.val_dataloadr()\n",
    "        self.num_train_batches = (len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49118415-277b-48fc-a924-534141d5ccc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# update"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
